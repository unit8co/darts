{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TiDE\n",
    "This notebook walks through how to use TiDE and benchmarks it against N-HiTS.\n",
    "\n",
    "TiDE (Time-series Dense Encoder) is a pure DL encoder-decoder architecture. It is special in that the temporal decoder can help mitigate the effects of anomalous samples on a forecast (Fig. 4 in the paper).\n",
    "\n",
    "See the original paper and model description here: [http://arxiv.org/abs/2304.08424](http://arxiv.org/abs/2304.08424)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix python path if working locally\n",
    "from utils import fix_pythonpath_if_working_locally\n",
    "\n",
    "fix_pythonpath_if_working_locally()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from darts.models import NHiTSModel, TiDEModel\n",
    "from darts.datasets import AusBeerDataset\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from darts.metrics import mae, mse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameter Setup\n",
    "Boilerplate code is no fun, especially in the context of training multiple models to compare performance. To avoid this, common configuration values are stored to be re-used by the different models.\n",
    "\n",
    "A few interesting things about these parameters:\n",
    "1. Gradient clipping\n",
    "\n",
    "This mitigate exploding gradients during backpropagation. This allows for us to set an upper limit on the gradient for a batch.\n",
    "\n",
    "2. Learning rate \n",
    "\n",
    "The majority of the learning done by a model is in the earlier epochs. As training goes on it is often helpful to reduce the learning rate to fine-tine the model. That being said, it can also lead to significant overfitting.\n",
    "\n",
    "3. Early stopping\n",
    "\n",
    "To avoid overfitting the model, early stopping is used. This allows for the validation to be monitored and for training to be stopped based on some preset conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_kwargs = {\n",
    "    \"lr\": 1e-3,\n",
    "}\n",
    "\n",
    "pl_trainer_kwargs = {\n",
    "    \"gradient_clip_val\": 1,\n",
    "    \"max_epochs\": 200,\n",
    "    \"accelerator\": \"auto\",\n",
    "    \"callbacks\": [],\n",
    "}\n",
    "\n",
    "lr_scheduler_cls = torch.optim.lr_scheduler.ExponentialLR\n",
    "lr_scheduler_kwargs = {\n",
    "    \"gamma\": 0.999,\n",
    "}\n",
    "\n",
    "early_stopping_args = {\n",
    "    \"monitor\": \"val_loss\",\n",
    "    \"patience\": 10,\n",
    "    \"min_delta\": 1e-3,\n",
    "    \"mode\": \"min\",\n",
    "}\n",
    "\n",
    "common_model_args = {\n",
    "    \"input_chunk_length\": 12,\n",
    "    \"output_chunk_length\": 12,\n",
    "    \"optimizer_kwargs\": optimizer_kwargs,\n",
    "    \"pl_trainer_kwargs\": pl_trainer_kwargs,\n",
    "    \"lr_scheduler_cls\": lr_scheduler_cls,\n",
    "    \"lr_scheduler_kwargs\": lr_scheduler_kwargs,\n",
    "    \"likelihood\": None,\n",
    "    \"save_checkpoints\": True,\n",
    "    \"batch_size\": 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and preparation\n",
    "We consider the Australian quarterly beer sales in megaliters. \n",
    "\n",
    "Before training, we split the data into train, validation, and test sets. The model will learn from the train set, use the validation set to determine when to stop training, and finally be evaluated on the test set.\n",
    "\n",
    "To avoid leaking information from the validation and test sets, we scale the data based on the properties of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = AusBeerDataset().load()\n",
    "\n",
    "train, temp = series.split_after(0.6)\n",
    "val, test = temp.split_after(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.plot(label=\"train\")\n",
    "val.plot(label=\"val\")\n",
    "test.plot(label=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler()  # Scaler(StandardScaler())\n",
    "train = scaler.fit_transform(train)\n",
    "val = scaler.transform(val)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configuration\n",
    "Using the already established shared arguments, we can see that the default parameters for NHiTS and TiDE are used. The only exception is that TiDE is tested both with and without Reversible Instance Normalization.\n",
    "\n",
    "We then iterate through the model dictionary and train all of the models. When using early stopping it is important to save checkpoints. This allows for us to continue past the best model configuration and then restore the optimal weights once training has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nhits = NHiTSModel(\n",
    "    **common_model_args,\n",
    ")\n",
    "\n",
    "model_tide = TiDEModel(\n",
    "    **common_model_args,\n",
    "    use_reversible_instance_norm=False,\n",
    ")\n",
    "\n",
    "model_tide_rin = TiDEModel(\n",
    "    **common_model_args,\n",
    "    use_reversible_instance_norm=True,\n",
    ")\n",
    "\n",
    "model_list = {\n",
    "    \"NHiTS\": model_nhits,\n",
    "    \"TiDE\": model_tide,\n",
    "    \"TiDE+RIN\": model_tide_rin,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_list.values():\n",
    "\n",
    "    # early stopping needs to get reset for each model\n",
    "    pl_trainer_kwargs[\"callbacks\"] = [\n",
    "        EarlyStopping(\n",
    "            **early_stopping_args,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        series=train,\n",
    "        val_series=val,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    model.load_from_checkpoint(model_name=model.model_name, best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_series = scaler.transform(series)\n",
    "pred_steps = common_model_args[\"output_chunk_length\"] * 2\n",
    "pred_input = test[:-pred_steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.figure(figsize=(15, 5)), plt.gca()\n",
    "\n",
    "pred_input.plot(label=\"input\")\n",
    "\n",
    "\n",
    "test[-pred_steps:].plot(label=\"ground truth\", ax=ax)\n",
    "\n",
    "result_accumulator = {}\n",
    "\n",
    "for model_name, model in model_list.items():\n",
    "    pred_series = model.predict(n=pred_steps, series=pred_input)\n",
    "    pred_series.plot(label=model_name, ax=ax)\n",
    "\n",
    "    result_accumulator[model_name] = {\n",
    "        \"mae\": mae(scaled_series[-pred_steps:], pred_series),\n",
    "        \"mse\": mse(scaled_series[-pred_steps:], pred_series),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "In this case, TiDE is shown to outperform NHiTS. Inclusion of reversible instance normalization (RIN) helps reduce the forecasting error as well; however, it is not always going to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(result_accumulator, orient=\"index\")\n",
    "results_df.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "A concern when using model checkpointing is that a significant amount of disk space can be used when training a large number of models. Be sure to cleanup when you no longer need your model artifacts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree(\"darts_logs\")\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
