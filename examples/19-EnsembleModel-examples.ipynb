{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling Models\n",
    "\n",
    "The following is a brief demonstration of the ensemble models in Darts. Starting from the examples provided in the [Quickstart Notebook](https://unit8co.github.io/darts/quickstart/00-quickstart.html), some advanced features and subtilities will be detailed.\n",
    "\n",
    "The following topics are covered in this notebook :\n",
    "* [Basics & references](#Basics-&-references)\n",
    "* [Naive Ensembling](#naive-ensembling)\n",
    "    * [Deterministic](#naive-ensembling)\n",
    "    * [Covariates & multivariate series](#using-covariates--predicting-multivariate-series)\n",
    "    * [Probabilistic](#probabilistic-naive-ensembling)\n",
    "* [Learned Ensembling](#learned-ensembling)\n",
    "    * [Deterministic](#learned-ensembling)\n",
    "    * [Probabilistic](#probabilistic-regression-ensemble)\n",
    "    * [Bootstraping](#bootstrapping-regression-ensemble)\n",
    "* [Pre-trained Ensembling](#pre-trained-ensembling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix python path if working locally\n",
    "from utils import fix_pythonpath_if_working_locally\n",
    "\n",
    "fix_pythonpath_if_working_locally()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import (\n",
    "    ExponentialSmoothing,\n",
    "    KalmanForecaster,\n",
    "    LinearRegressionModel,\n",
    "    NaiveDrift,\n",
    "    NaiveEnsembleModel,\n",
    "    NaiveSeasonal,\n",
    "    RandomForest,\n",
    "    RegressionEnsembleModel,\n",
    "    TCNModel,\n",
    ")\n",
    "from darts.metrics import mape\n",
    "from darts.datasets import AirPassengersDataset\n",
    "from darts.utils.timeseries_generation import (\n",
    "    linear_timeseries,\n",
    "    datetime_attribute_timeseries as dt_attr,\n",
    ")\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics & references\n",
    "\n",
    "Ensembling combines the forecasts of several \"weak\" models to obtain a more robust and accurate model.\n",
    "\n",
    "All of Darts' ensembling models rely on the **stacking technique** ([reference](https://machinelearningmastery.com/essence-of-stacking-ensembles-for-machine-learning/)). They provide the same functionalities as the other forecasting models. Depending on the ensembled models, they can:\n",
    "\n",
    "* levarage covariates\n",
    "* be trained on multiple-series\n",
    "* predict multivariate targets\n",
    "* generate probabilistic forecasts\n",
    "* and more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the AirPassenger dataset, directly available in darts\n",
    "ts_air = AirPassengersDataset().load()\n",
    "ts_air.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Ensembling\n",
    "\n",
    "Naive ensembling simply takes the average (mean) of the forecasts generated by the ensembled forecasting models. Darts' `NaiveEnsembleModel` accepts both local and global forecasting models (as well as combination of the two, with some additional limitations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_ensemble = NaiveEnsembleModel(\n",
    "    forecasting_models=[NaiveSeasonal(K=12), NaiveDrift()]\n",
    ")\n",
    "\n",
    "backtest = naive_ensemble.historical_forecasts(ts_air, start=0.6, forecast_horizon=3)\n",
    "\n",
    "ts_air.plot(label=\"series\")\n",
    "backtest.plot(label=\"prediction\")\n",
    "print(\"NaiveEnsemble (naive) MAPE:\", round(mape(backtest, ts_air), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: after looking at the each model's MAPE, one would notice that `NaiveSeasonal` is actually performing better on its own than ensembled with `NaiveDrift`. Checking the performance of the single models is in general a good practice before defining an ensemble. \n",
    "\n",
    "Before creating the new `NaiveEnsemble`, we will screen models to identify which ones would do well together. The candidates are : \n",
    "- `LinearRegressionModel` : classic and simple model\n",
    "- `ExponentialSmoothing` : moving window model\n",
    "- `FalmanForecaster` : a filter-based model\n",
    "- `RandomForest` : decision trees model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_models = {\n",
    "    \"LinearRegression\": (LinearRegressionModel, {\"lags\": 12}),\n",
    "    \"ExponentialSmoothing\": (ExponentialSmoothing, {}),\n",
    "    \"KalmanForecaster\": (KalmanForecaster, {\"dim_x\": 12}),\n",
    "    \"RandomForest\": (RandomForest, {\"lags\": 12, \"random_state\": 0}),\n",
    "}\n",
    "\n",
    "backtest_models = []\n",
    "\n",
    "for model_name, (model_cls, model_kwargs) in candidates_models.items():\n",
    "    model = model_cls(**model_kwargs)\n",
    "    backtest_models.append(\n",
    "        model.historical_forecasts(ts_air, start=0.6, forecast_horizon=3)\n",
    "    )\n",
    "    print(f\"{model_name} MAPE: {round(mape(backtest_models[-1], ts_air), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(2, 2, figsize=(9, 6))\n",
    "for ax, backtest, model_name in zip(\n",
    "    axes.flatten(),\n",
    "    backtest_models,\n",
    "    list(candidates_models.keys()),\n",
    "):\n",
    "    ts_air[-len(backtest) :].plot(ax=ax, label=\"ground truth\")\n",
    "    backtest.plot(ax=ax, label=model_name)\n",
    "\n",
    "    ax.set_title(model_name)\n",
    "    ax.set_ylim([250, 650])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The historical forecasts obtained with the `LinearRegressionModel` and `KalmanForecaster` look quite similar whereas `ExponentialSmoothing` tends to understimate the true values and `RandomForest` is failing to capture the peaks. To benefits from the ensemble, we will favor diversity and continue with the `LinearRegressionModel` and `ExponentialSmoothing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NaiveEnsembleModel(\n",
    "    forecasting_models=[LinearRegressionModel(lags=12), ExponentialSmoothing()]\n",
    ")\n",
    "\n",
    "backtest = ensemble.historical_forecasts(ts_air, start=0.6, forecast_horizon=3)\n",
    "\n",
    "ts_air[-len(backtest) :].plot(label=\"series\")\n",
    "backtest.plot(label=\"prediction\")\n",
    "plt.ylim([250, 650])\n",
    "print(\"NaiveEnsemble (v2) MAPE:\", round(mape(backtest, ts_air), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the individual model MAPE score, 4.64008 for the `LinearRegressionModel` and 4.44874 for the `ExponentialSmoothing`, the ensembling improved the accuracy to 4.04297!\n",
    "\n",
    "\n",
    "### Using covariates & predicting multivariate series\n",
    "\n",
    "Depending on the forecasting models used, the `EnsembleModel` can of course also leverage covariates or forecast multivariates series! The covariates will be passed only to the forecasting models supporting them.\n",
    "\n",
    "In the example below, the `ExponentialSmoothing` model does not support any covariates whereas the `LinearRegressionModel` model supports `future_covariates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NaiveEnsembleModel(\n",
    "    [LinearRegressionModel(lags=12, lags_future_covariates=[0]), ExponentialSmoothing()]\n",
    ")\n",
    "\n",
    "# encoding the months as integer, normalised\n",
    "future_cov = dt_attr(ts_air.time_index, \"month\", add_length=12) / 12\n",
    "backtest = ensemble.historical_forecasts(\n",
    "    ts_air, future_covariates=future_cov, start=0.6, forecast_horizon=3\n",
    ")\n",
    "\n",
    "ts_air[-len(backtest) :].plot(label=\"series\")\n",
    "backtest.plot(label=\"prediction\")\n",
    "plt.ylim([250, 650])\n",
    "print(\"NaiveEnsemble (w/ future covariates) MAPE:\", round(mape(backtest, ts_air), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Probabilistic naive ensembling\n",
    "\n",
    "Combining models supporting probabilistic forecasts results in a probabilistic `NaiveEnsembleModel`! We can easily tweak the models used above to make them probabilistic and obtain confidence interval in ours forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_probabilistic = NaiveEnsembleModel(\n",
    "    forecasting_models=[\n",
    "        LinearRegressionModel(\n",
    "            lags=12,\n",
    "            likelihood=\"quantile\",\n",
    "            quantiles=[0.05, 0.5, 0.95],\n",
    "        ),\n",
    "        ExponentialSmoothing(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# must pass num_samples > 1 to obtain a probabilistic forecasts\n",
    "backtest = ensemble_probabilistic.historical_forecasts(\n",
    "    ts_air, start=0.6, forecast_horizon=3, num_samples=100\n",
    ")\n",
    "\n",
    "ts_air[-len(backtest) :].plot(label=\"ground truth\")\n",
    "backtest.plot(label=\"prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned Ensembling\n",
    "\n",
    "Ensembling can also be considered as a supervised regression problem: given a set of forecasts (features), find a model that combines them in order to minimise errors on the target. This is what the `RegressionEnsembleModel` does. The main three parameters are:\n",
    "\n",
    "* `forecasting_models` is a list of forecasting models whose predictions we want to ensemble.\n",
    "* `regression_train_n_points` is the number of time steps to use for fitting the \"ensemble regression\" model (i.e., the inner model that combines the forecasts).\n",
    "* `regression_model` is, optionally, a sklearn-compatible regression model or a Darts `RegressionModel` to be used for the ensemble regression. If not specified, Darts' `LinearRegressionModel` is used. Using a sklearn model is easy out-of-the-box, but using one of Darts' regression models allows to potentially take arbitrary lags of the individual forecasts as inputs of the regression model.\n",
    "\n",
    "Once these elements are in place, a `RegressionEnsembleModel` can be used like a regular forecasting model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = RegressionEnsembleModel(\n",
    "    forecasting_models=[NaiveSeasonal(K=12), NaiveDrift()],\n",
    "    regression_train_n_points=12,\n",
    ")\n",
    "\n",
    "backtest = ensemble_model.historical_forecasts(ts_air, start=0.6, forecast_horizon=3)\n",
    "\n",
    "ts_air.plot()\n",
    "backtest.plot()\n",
    "plt.show()\n",
    "\n",
    "print(\"RegressionEnsemble (naive) MAPE:\", round(mape(backtest, ts_air), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the MAPE of 11.87818 obtained at the beginning of the [naive ensembling section](#Naive-ensembling), adding a `LinearRegressionModel` on top of the two naive models does improve the quality of the forecast.\n",
    "\n",
    "Now, let's see if we can observe similar gain when the `RegressionEnsemble` forecasting models are not naive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = RegressionEnsembleModel(\n",
    "    forecasting_models=[LinearRegressionModel(lags=12), ExponentialSmoothing()],\n",
    "    regression_train_n_points=12,\n",
    ")\n",
    "\n",
    "backtest = ensemble.historical_forecasts(ts_air, start=0.6, forecast_horizon=3)\n",
    "\n",
    "ts_air.plot(label=\"series\")\n",
    "backtest.plot(label=\"prediction\")\n",
    "plt.show()\n",
    "print(\"RegressionEnsemble (v2) MAPE:\", round(mape(backtest, ts_air), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, even if the MAPE improved compared to the `RegressionEnsemble` relying on naive models (MAPE: 4.85142), it does not outperform the `NaiveEnsemble` using similar forecasting models (MAPE: 4.04297). \n",
    "\n",
    "This performance gap is partially caused by the points set aside to train the ensembling `LinearRegression`; the two forecasting models (`LinearRegression` and `ExponentialSmoothing`) cannot access the latest values of the series, which contains a marked upward trend.\n",
    "\n",
    "Out of curiosity, we can use the `Ridge` regression model from the sklearn library to ensemble the forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ensemble = RegressionEnsembleModel(\n",
    "    forecasting_models=[LinearRegressionModel(lags=12), ExponentialSmoothing()],\n",
    "    regression_train_n_points=12,\n",
    "    regression_model=Ridge(),\n",
    ")\n",
    "\n",
    "backtest = ensemble.historical_forecasts(ts_air, start=0.6, forecast_horizon=3)\n",
    "\n",
    "print(\"RegressionEnsemble (Ridge) MAPE:\", round(mape(backtest, ts_air), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particuliar scenario, using a regression model with a regularization term deteriorated the forecasts but there might be other cases where it will improve them.\n",
    "\n",
    "### Training using historical forecasts\n",
    "\n",
    "When predicting a number of values greater than their `output_chunk_length`, `GlobalForecastingModels` rely on auto-regression (use their own output as input) to forecast values far in the future. However, the quality of the forecasts can considerably decrease as the predicted timestamp get further from the end of the observations. During `RegressionEnsemble`'s regression model training, the forecasting models generate forecasts for timestamps where the ground truth is actually known and available, making it possible to use `historical_forecasts` instead of `predict()`.\n",
    "\n",
    "**Note**: In order to ensure that `regression_train_n_points` are generated, the `output_chunk_length` of all the forecasting models must be multiple of `regression_train_n_points`. If the forecasting models expect covariates, they should extend far enough into the future. If these two conditions are not met, a warning message will be displayed and the regression model of the ensemble might be trained with less points than specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the ExponentialSmoothing (local) with RandomForest (global)\n",
    "ensemble = RegressionEnsembleModel(\n",
    "    forecasting_models=[\n",
    "        LinearRegressionModel(lags=12),\n",
    "        RandomForest(lags=12, random_state=0),\n",
    "    ],\n",
    "    regression_train_n_points=12,\n",
    "    train_using_historical_forecasts=False,\n",
    ")\n",
    "backtest = ensemble.historical_forecasts(ts_air, start=0.6, forecast_horizon=3)\n",
    "\n",
    "ensemble_hist_fct = RegressionEnsembleModel(\n",
    "    forecasting_models=[\n",
    "        LinearRegressionModel(lags=12),\n",
    "        RandomForest(lags=12, random_state=0),\n",
    "    ],\n",
    "    regression_train_n_points=12,\n",
    "    train_using_historical_forecasts=True,\n",
    ")\n",
    "backtest_hist_fct = ensemble_hist_fct.historical_forecasts(\n",
    "    ts_air, start=0.6, forecast_horizon=3\n",
    ")\n",
    "\n",
    "print(\"RegressionEnsemble (no hist_fct) MAPE:\", round(mape(backtest, ts_air), 5))\n",
    "print(\"RegressionEnsemble (hist_fct) MAPE:\", round(mape(backtest_hist_fct, ts_air), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, using historical forecast with the forecasting models to the train the regression model produce better forecasts.\n",
    "\n",
    "### Probabilistic regression ensemble\n",
    "\n",
    "In order to be probabilistic, the `RegressionEnsembleModel`, must have a probabilistic ensembling regression model (see table in the README):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = RegressionEnsembleModel(\n",
    "    forecasting_models=[LinearRegressionModel(lags=12), ExponentialSmoothing()],\n",
    "    regression_train_n_points=12,\n",
    "    regression_model=LinearRegressionModel(\n",
    "        lags_future_covariates=[0], likelihood=\"quantile\", quantiles=[0.05, 0.5, 0.95]\n",
    "    ),\n",
    ")\n",
    "\n",
    "backtest = ensemble.historical_forecasts(\n",
    "    ts_air, start=0.6, forecast_horizon=3, num_samples=100\n",
    ")\n",
    "\n",
    "ts_air[-len(backtest) :].plot(label=\"ground truth\")\n",
    "backtest.plot(label=\"prediction\")\n",
    "\n",
    "print(\"RegressionEnsemble (probabilistic) MAPE:\", round(mape(backtest, ts_air), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping regression ensemble\n",
    "\n",
    "When the forecasting models of a `RegressionEnsembleModel` are probabilistic, the samples dimension of their forecasts is reduced and used as covariates for the ensembling regression. Since the ensembling regression model is deterministic, the generated forecasts is deterministic as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = RegressionEnsembleModel(\n",
    "    forecasting_models=[\n",
    "        LinearRegressionModel(\n",
    "            lags=12, likelihood=\"quantile\", quantiles=[0.05, 0.5, 0.95]\n",
    "        ),\n",
    "        ExponentialSmoothing(),\n",
    "    ],\n",
    "    regression_train_n_points=12,\n",
    "    regression_train_num_samples=100,\n",
    "    regression_train_samples_reduction=\"median\",\n",
    ")\n",
    "\n",
    "backtest = ensemble.historical_forecasts(ts_air, start=0.6, forecast_horizon=3)\n",
    "\n",
    "ts_air[-len(backtest) :].plot(label=\"ground truth\")\n",
    "backtest.plot(label=\"prediction\")\n",
    "plt.show()\n",
    "\n",
    "print(\"RegressionEnsemble (bootstrap) MAPE:\", round(mape(backtest, ts_air), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Ensembling\n",
    "\n",
    "As both `NaiveEnsembleModel` and `RegressionEnsembleModel` accept `GlobalForecastingModel` as forecasting models, they can be used to ensemble pre-trained deep learning and regression models. Note that this functionnality is only supported if all the ensembled forecasting models are instances from the `GlobalForecastingModel` class and are already trained when creating the ensemble.\n",
    "\n",
    "**Disclaimer** : Be careful not to pre-train the models with data used during validation as this would introduce considerable bias.\n",
    "\n",
    "**Note** : The parameters for the `TCNModel` is heavily inspired from the [TCNModel example notebook](https://unit8co.github.io/darts/examples/05-TCN-examples.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holding out values for validation\n",
    "train, val = ts_air.split_after(0.8)\n",
    "\n",
    "# scaling the target\n",
    "scaler = Scaler()\n",
    "train = scaler.fit_transform(train)\n",
    "val = scaler.transform(val)\n",
    "\n",
    "# use the month as a covariate\n",
    "month_series = dt_attr(ts_air.time_index, attribute=\"month\", one_hot=True)\n",
    "scaler_month = Scaler()\n",
    "month_series = scaler_month.fit_transform(month_series)\n",
    "\n",
    "# training a regular linear regression, without any covariates\n",
    "linreg_model = LinearRegressionModel(lags=24)\n",
    "linreg_model.fit(train)\n",
    "\n",
    "# instanciating a TCN model with parameters optimized for the AirPassenger dataset\n",
    "tcn_model = TCNModel(\n",
    "    input_chunk_length=24,\n",
    "    output_chunk_length=12,\n",
    "    n_epochs=500,\n",
    "    dilation_base=2,\n",
    "    weight_norm=True,\n",
    "    kernel_size=5,\n",
    "    num_filters=3,\n",
    "    random_state=0,\n",
    ")\n",
    "tcn_model.fit(train, past_covariates=month_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we will look at the forecast of the model taken individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual model forecasts\n",
    "pred_linreg = linreg_model.predict(24)\n",
    "pred_tcn = tcn_model.predict(24, verbose=False)\n",
    "\n",
    "# scaling them back\n",
    "pred_linreg_rescaled = scaler.inverse_transform(pred_linreg)\n",
    "pred_tcn_rescaled = scaler.inverse_transform(pred_tcn)\n",
    "\n",
    "# plotting\n",
    "ts_air[-24:].plot(label=\"ground truth\")\n",
    "pred_linreg_rescaled.plot(label=\"LinearRegressionModel\")\n",
    "pred_tcn_rescaled.plot(label=\"TCNModel\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a good idea of the individual performance of each of these models, we can ensemble them. We must make sure to set `retrain_forecasting_models=False` or the ensemble will need to be fitted before being able to call `predict()`.\n",
    "\n",
    "**Advice** : Use the `save()` method to export your model and keep a copy of your weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_ensemble = NaiveEnsembleModel(\n",
    "    forecasting_models=[tcn_model, linreg_model], train_forecasting_models=False\n",
    ")\n",
    "# NaiveEnsemble initialized with pre-trained models can call predict() directly,\n",
    "# the `series` argument must however be provided\n",
    "pred_naive = naive_ensemble.predict(len(val), train)\n",
    "\n",
    "pretrain_ensemble = RegressionEnsembleModel(\n",
    "    forecasting_models=[tcn_model, linreg_model],\n",
    "    regression_train_n_points=24,\n",
    "    train_forecasting_models=False,\n",
    "    train_using_historical_forecasts=False,\n",
    ")\n",
    "# RegressionEnsemble must train the ensemble model, even if the forecasting models are already trained\n",
    "pretrain_ensemble.fit(train)\n",
    "pred_ens = pretrain_ensemble.predict(len(val))\n",
    "\n",
    "# scaling back the predictions\n",
    "pred_naive_rescaled = scaler.inverse_transform(pred_naive)\n",
    "pred_ens_rescaled = scaler.inverse_transform(pred_ens)\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(8, 5))\n",
    "scaler.inverse_transform(val).plot(label=\"ground truth\")\n",
    "pred_naive_rescaled.plot(label=\"pre-trained NaiveEnsemble\")\n",
    "pred_ens_rescaled.plot(label=\"pre-trained RegressionEnsemble\")\n",
    "plt.ylim([250, 650])\n",
    "\n",
    "# MAPE\n",
    "print(\"LinearRegression MAPE:\", round(mape(pred_linreg_rescaled, ts_air), 5))\n",
    "print(\"TCNModel MAPE:\", round(mape(pred_tcn_rescaled, ts_air), 5))\n",
    "print(\"NaiveEnsemble (pre-trained) MAPE:\", round(mape(pred_naive_rescaled, ts_air), 5))\n",
    "print(\n",
    "    \"RegressionEnsemble (pre-trained) MAPE:\", round(mape(pred_ens_rescaled, ts_air), 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ensembling pre-trained `LinearRegression` and `TCNModel` models allowed us to out-perform single models and training a linear regression on top of these two models forecasts further improved the MAPE score.\n",
    "\n",
    "If the gains remain limited on this small dataset, ensembling is a powerful technique that can yield impressive results and was notably used by the winners of the 4th edition of the Makridakis Competition ([website](https://mofc.unic.ac.cy/history-of-competitions/), [github repository](https://github.com/Mcompetitions/))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
