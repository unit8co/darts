
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torch.nn.modules.normalization &#8212; darts  documentation</title>
    
  <link href="../../../../_static/css/theme.css" rel="stylesheet">
  <link href="../../../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <link rel="shortcut icon" href="../../../../_static/docs-favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../../index.html">
  <img src="../../../../_static/darts-logo-trim.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../README.html">
  Home
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../quickstart/00-quickstart.html">
  Quickstart
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../userguide.html">
  User Guide
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../generated_api/darts.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../examples.html">
  Examples
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../release_notes/RELEASE_NOTES.html">
  Release Notes
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/unit8co/darts" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/unit8co" rel="noopener" target="_blank" title="Twitter">
            <span><i class="fab fa-twitter-square"></i></span>
            <label class="sr-only">Twitter</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for torch.nn.modules.normalization</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-defs</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numbers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Size</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span><span class="p">,</span> <span class="n">init</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.parameter</span><span class="w"> </span><span class="kn">import</span> <span class="n">Parameter</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">._functions</span><span class="w"> </span><span class="kn">import</span> <span class="n">CrossMapLRN2d</span> <span class="k">as</span> <span class="n">_cross_map_lrn2d</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.module</span><span class="w"> </span><span class="kn">import</span> <span class="n">Module</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;LocalResponseNorm&quot;</span><span class="p">,</span> <span class="s2">&quot;CrossMapLRN2d&quot;</span><span class="p">,</span> <span class="s2">&quot;LayerNorm&quot;</span><span class="p">,</span> <span class="s2">&quot;GroupNorm&quot;</span><span class="p">,</span> <span class="s2">&quot;RMSNorm&quot;</span><span class="p">]</span>


<span class="k">class</span><span class="w"> </span><span class="nc">LocalResponseNorm</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies local response normalization over an input signal.</span>

<span class="sd">    The input signal is composed of several input planes, where channels occupy the second dimension.</span>
<span class="sd">    Applies normalization across channels.</span>

<span class="sd">    .. math::</span>
<span class="sd">        b_{c} = a_{c}\left(k + \frac{\alpha}{n}</span>
<span class="sd">        \sum_{c&#39;=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c&#39;}^2\right)^{-\beta}</span>

<span class="sd">    Args:</span>
<span class="sd">        size: amount of neighbouring channels used for normalization</span>
<span class="sd">        alpha: multiplicative factor. Default: 0.0001</span>
<span class="sd">        beta: exponent. Default: 0.75</span>
<span class="sd">        k: additive factor. Default: 1</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C, *)`</span>
<span class="sd">        - Output: :math:`(N, C, *)` (same shape as input)</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; lrn = nn.LocalResponseNorm(2)</span>
<span class="sd">        &gt;&gt;&gt; signal_2d = torch.randn(32, 5, 24, 24)</span>
<span class="sd">        &gt;&gt;&gt; signal_4d = torch.randn(16, 5, 7, 7, 7, 7)</span>
<span class="sd">        &gt;&gt;&gt; output_2d = lrn(signal_2d)</span>
<span class="sd">        &gt;&gt;&gt; output_4d = lrn(signal_4d)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;size&quot;</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="s2">&quot;k&quot;</span><span class="p">]</span>
    <span class="n">size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">local_response_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">{size}</span><span class="s2">, alpha=</span><span class="si">{alpha}</span><span class="s2">, beta=</span><span class="si">{beta}</span><span class="s2">, k=</span><span class="si">{k}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CrossMapLRN2d</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_cross_map_lrn2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">{size}</span><span class="s2">, alpha=</span><span class="si">{alpha}</span><span class="s2">, beta=</span><span class="si">{beta}</span><span class="s2">, k=</span><span class="si">{k}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>


<span class="n">_shape_t</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">Size</span><span class="p">]</span>


<span class="k">class</span><span class="w"> </span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Layer Normalization over a mini-batch of inputs.</span>

<span class="sd">    This layer implements the operation as described in</span>
<span class="sd">    the paper `Layer Normalization &lt;https://arxiv.org/abs/1607.06450&gt;`__</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta</span>

<span class="sd">    The mean and standard-deviation are calculated over the last `D` dimensions, where `D`</span>
<span class="sd">    is the dimension of :attr:`normalized_shape`. For example, if :attr:`normalized_shape`</span>
<span class="sd">    is ``(3, 5)`` (a 2-dimensional shape), the mean and standard-deviation are computed over</span>
<span class="sd">    the last 2 dimensions of the input (i.e. ``input.mean((-2, -1))``).</span>
<span class="sd">    :math:`\gamma` and :math:`\beta` are learnable affine transform parameters of</span>
<span class="sd">    :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.</span>
<span class="sd">    The variance is calculated via the biased estimator, equivalent to</span>
<span class="sd">    `torch.var(input, unbiased=False)`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Unlike Batch Normalization and Instance Normalization, which applies</span>
<span class="sd">        scalar scale and bias for each entire channel/plane with the</span>
<span class="sd">        :attr:`affine` option, Layer Normalization applies per-element scale and</span>
<span class="sd">        bias with :attr:`elementwise_affine`.</span>

<span class="sd">    This layer uses statistics computed from input data in both training and</span>
<span class="sd">    evaluation modes.</span>

<span class="sd">    Args:</span>
<span class="sd">        normalized_shape (int or list or torch.Size): input shape from an expected input</span>
<span class="sd">            of size</span>

<span class="sd">            .. math::</span>
<span class="sd">                [* \times \text{normalized\_shape}[0] \times \text{normalized\_shape}[1]</span>
<span class="sd">                    \times \ldots \times \text{normalized\_shape}[-1]]</span>

<span class="sd">            If a single integer is used, it is treated as a singleton list, and this module will</span>
<span class="sd">            normalize over the last dimension which is expected to be of that specific size.</span>
<span class="sd">        eps: a value added to the denominator for numerical stability. Default: 1e-5</span>
<span class="sd">        elementwise_affine: a boolean value that when set to ``True``, this module</span>
<span class="sd">            has learnable per-element affine parameters initialized to ones (for weights)</span>
<span class="sd">            and zeros (for biases). Default: ``True``.</span>
<span class="sd">        bias: If set to ``False``, the layer will not learn an additive bias (only relevant if</span>
<span class="sd">            :attr:`elementwise_affine` is ``True``). Default: ``True``.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight: the learnable weights of the module of shape</span>
<span class="sd">            :math:`\text{normalized\_shape}` when :attr:`elementwise_affine` is set to ``True``.</span>
<span class="sd">            The values are initialized to 1.</span>
<span class="sd">        bias:   the learnable bias of the module of shape</span>
<span class="sd">                :math:`\text{normalized\_shape}` when :attr:`elementwise_affine` is set to ``True``.</span>
<span class="sd">                The values are initialized to 0.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *)`</span>
<span class="sd">        - Output: :math:`(N, *)` (same shape as input)</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # NLP Example</span>
<span class="sd">        &gt;&gt;&gt; batch, sentence_length, embedding_dim = 20, 5, 10</span>
<span class="sd">        &gt;&gt;&gt; embedding = torch.randn(batch, sentence_length, embedding_dim)</span>
<span class="sd">        &gt;&gt;&gt; layer_norm = nn.LayerNorm(embedding_dim)</span>
<span class="sd">        &gt;&gt;&gt; # Activate module</span>
<span class="sd">        &gt;&gt;&gt; layer_norm(embedding)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Image Example</span>
<span class="sd">        &gt;&gt;&gt; N, C, H, W = 20, 5, 10, 10</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(N, C, H, W)</span>
<span class="sd">        &gt;&gt;&gt; # Normalize over the last three dimensions (i.e. the channel and spatial dimensions)</span>
<span class="sd">        &gt;&gt;&gt; # as shown in the image below</span>
<span class="sd">        &gt;&gt;&gt; layer_norm = nn.LayerNorm([C, H, W])</span>
<span class="sd">        &gt;&gt;&gt; output = layer_norm(input)</span>

<span class="sd">    .. image:: ../_static/img/nn/layer_norm.jpg</span>
<span class="sd">        :scale: 50 %</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;normalized_shape&quot;</span><span class="p">,</span> <span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="s2">&quot;elementwise_affine&quot;</span><span class="p">]</span>
    <span class="n">normalized_shape</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">elementwise_affine</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">normalized_shape</span><span class="p">:</span> <span class="n">_shape_t</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">elementwise_affine</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">):</span>
            <span class="c1"># mypy error: incompatible types in assignment</span>
            <span class="n">normalized_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">normalized_shape</span><span class="p">,)</span>  <span class="c1"># type: ignore[assignment]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalized_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">normalized_shape</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">elementwise_affine</span> <span class="o">=</span> <span class="n">elementwise_affine</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">elementwise_affine</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

<div class="viewcode-block" id="LayerNorm.reset_parameters"><a class="viewcode-back" href="../../../../generated_api/darts.models.components.layer_norm_variants.html#darts.models.components.layer_norm_variants.LayerNorm.reset_parameters">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">elementwise_affine</span><span class="p">:</span>
            <span class="n">init</span><span class="o">.</span><span class="n">ones_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span></div>

<div class="viewcode-block" id="LayerNorm.forward"><a class="viewcode-back" href="../../../../generated_api/darts.models.components.layer_norm_variants.html#darts.models.components.layer_norm_variants.LayerNorm.forward">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="LayerNorm.extra_repr"><a class="viewcode-back" href="../../../../generated_api/darts.models.components.layer_norm_variants.html#darts.models.components.layer_norm_variants.LayerNorm.extra_repr">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="s2">&quot;</span><span class="si">{normalized_shape}</span><span class="s2">, eps=</span><span class="si">{eps}</span><span class="s2">, &quot;</span>
            <span class="s2">&quot;elementwise_affine=</span><span class="si">{elementwise_affine}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
        <span class="p">)</span></div>


<span class="k">class</span><span class="w"> </span><span class="nc">GroupNorm</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Group Normalization over a mini-batch of inputs.</span>

<span class="sd">    This layer implements the operation as described in</span>
<span class="sd">    the paper `Group Normalization &lt;https://arxiv.org/abs/1803.08494&gt;`__</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta</span>

<span class="sd">    The input channels are separated into :attr:`num_groups` groups, each containing</span>
<span class="sd">    ``num_channels / num_groups`` channels. :attr:`num_channels` must be divisible by</span>
<span class="sd">    :attr:`num_groups`. The mean and standard-deviation are calculated</span>
<span class="sd">    separately over the each group. :math:`\gamma` and :math:`\beta` are learnable</span>
<span class="sd">    per-channel affine transform parameter vectors of size :attr:`num_channels` if</span>
<span class="sd">    :attr:`affine` is ``True``.</span>
<span class="sd">    The variance is calculated via the biased estimator, equivalent to</span>
<span class="sd">    `torch.var(input, unbiased=False)`.</span>

<span class="sd">    This layer uses statistics computed from input data in both training and</span>
<span class="sd">    evaluation modes.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_groups (int): number of groups to separate the channels into</span>
<span class="sd">        num_channels (int): number of channels expected in input</span>
<span class="sd">        eps: a value added to the denominator for numerical stability. Default: 1e-5</span>
<span class="sd">        affine: a boolean value that when set to ``True``, this module</span>
<span class="sd">            has learnable per-channel affine parameters initialized to ones (for weights)</span>
<span class="sd">            and zeros (for biases). Default: ``True``.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C, *)` where :math:`C=\text{num\_channels}`</span>
<span class="sd">        - Output: :math:`(N, C, *)` (same shape as input)</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 6, 10, 10)</span>
<span class="sd">        &gt;&gt;&gt; # Separate 6 channels into 3 groups</span>
<span class="sd">        &gt;&gt;&gt; m = nn.GroupNorm(3, 6)</span>
<span class="sd">        &gt;&gt;&gt; # Separate 6 channels into 6 groups (equivalent with InstanceNorm)</span>
<span class="sd">        &gt;&gt;&gt; m = nn.GroupNorm(6, 6)</span>
<span class="sd">        &gt;&gt;&gt; # Put all 6 channels into a single group (equivalent with LayerNorm)</span>
<span class="sd">        &gt;&gt;&gt; m = nn.GroupNorm(1, 6)</span>
<span class="sd">        &gt;&gt;&gt; # Activating the module</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;num_groups&quot;</span><span class="p">,</span> <span class="s2">&quot;num_channels&quot;</span><span class="p">,</span> <span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="s2">&quot;affine&quot;</span><span class="p">]</span>
    <span class="n">num_groups</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">num_channels</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">affine</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_groups</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">affine</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">num_channels</span> <span class="o">%</span> <span class="n">num_groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;num_channels must be divisible by num_groups&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span> <span class="o">=</span> <span class="n">num_groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_channels</span> <span class="o">=</span> <span class="n">num_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">affine</span> <span class="o">=</span> <span class="n">affine</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">affine</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">num_channels</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">num_channels</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">affine</span><span class="p">:</span>
            <span class="n">init</span><span class="o">.</span><span class="n">ones_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">{num_groups}</span><span class="s2">, </span><span class="si">{num_channels}</span><span class="s2">, eps=</span><span class="si">{eps}</span><span class="s2">, affine=</span><span class="si">{affine}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span>
        <span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">RMSNorm</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Root Mean Square Layer Normalization over a mini-batch of inputs.</span>

<span class="sd">    This layer implements the operation as described in</span>
<span class="sd">    the paper `Root Mean Square Layer Normalization &lt;https://arxiv.org/pdf/1910.07467.pdf&gt;`__</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_i = \frac{x_i}{\mathrm{RMS}(x)} * \gamma_i, \quad</span>
<span class="sd">        \text{where} \quad \text{RMS}(x) = \sqrt{\epsilon + \frac{1}{n} \sum_{i=1}^{n} x_i^2}</span>

<span class="sd">    The RMS is taken over the last ``D`` dimensions, where ``D``</span>
<span class="sd">    is the dimension of :attr:`normalized_shape`. For example, if :attr:`normalized_shape`</span>
<span class="sd">    is ``(3, 5)`` (a 2-dimensional shape), the RMS is computed over</span>
<span class="sd">    the last 2 dimensions of the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        normalized_shape (int or list or torch.Size): input shape from an expected input</span>
<span class="sd">            of size</span>

<span class="sd">            .. math::</span>
<span class="sd">                [* \times \text{normalized\_shape}[0] \times \text{normalized\_shape}[1]</span>
<span class="sd">                    \times \ldots \times \text{normalized\_shape}[-1]]</span>

<span class="sd">            If a single integer is used, it is treated as a singleton list, and this module will</span>
<span class="sd">            normalize over the last dimension which is expected to be of that specific size.</span>
<span class="sd">        eps: a value added to the denominator for numerical stability. Default: ``torch.finfo(x.dtype).eps``</span>
<span class="sd">        elementwise_affine: a boolean value that when set to ``True``, this module</span>
<span class="sd">            has learnable per-element affine parameters initialized to ones (for weights). Default: ``True``.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *)`</span>
<span class="sd">        - Output: :math:`(N, *)` (same shape as input)</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; rms_norm = nn.RMSNorm([2, 3])</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2, 2, 3)</span>
<span class="sd">        &gt;&gt;&gt; rms_norm(input)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;normalized_shape&quot;</span><span class="p">,</span> <span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="s2">&quot;elementwise_affine&quot;</span><span class="p">]</span>
    <span class="n">normalized_shape</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
    <span class="n">elementwise_affine</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">normalized_shape</span><span class="p">:</span> <span class="n">_shape_t</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">elementwise_affine</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">):</span>
            <span class="c1"># mypy error: incompatible types in assignment</span>
            <span class="n">normalized_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">normalized_shape</span><span class="p">,)</span>  <span class="c1"># type: ignore[assignment]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalized_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">normalized_shape</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">elementwise_affine</span> <span class="o">=</span> <span class="n">elementwise_affine</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">elementwise_affine</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Resets parameters based on their initialization used in __init__.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">elementwise_affine</span><span class="p">:</span>
            <span class="n">init</span><span class="o">.</span><span class="n">ones_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runs forward pass.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">rms_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Extra information about the module.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="s2">&quot;</span><span class="si">{normalized_shape}</span><span class="s2">, eps=</span><span class="si">{eps}</span><span class="s2">, &quot;</span>
            <span class="s2">&quot;elementwise_affine=</span><span class="si">{elementwise_affine}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
        <span class="p">)</span>


<span class="c1"># TODO: ContrastiveNorm2d</span>
<span class="c1"># TODO: DivisiveNorm2d</span>
<span class="c1"># TODO: SubtractiveNorm2d</span>
</pre></div>

              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2020 - 2025, Unit8 SA (Apache 2.0 License).<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 5.0.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>