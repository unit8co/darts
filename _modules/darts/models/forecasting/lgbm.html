
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>darts.models.forecasting.lgbm &#8212; darts  documentation</title>
    
  <link href="../../../../_static/css/theme.css" rel="stylesheet">
  <link href="../../../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <link rel="shortcut icon" href="../../../../_static/docs-favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../../index.html">
  <img src="../../../../_static/darts-logo-trim.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../README.html">
  Home
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../quickstart/00-quickstart.html">
  Quickstart
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../userguide.html">
  User Guide
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../generated_api/darts.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../examples.html">
  Examples
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../release_notes/RELEASE_NOTES.html">
  Release Notes
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/unit8co/darts" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/unit8co" rel="noopener" target="_blank" title="Twitter">
            <span><i class="fab fa-twitter-square"></i></span>
            <label class="sr-only">Twitter</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for darts.models.forecasting.lgbm</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">LightGBM Models</span>
<span class="sd">---------------</span>

<span class="sd">This module offers wrappers around LightGBM&#39;s Gradient Boosted Trees algorithms.</span>

<span class="sd">* :class:`~darts.models.forecasting.lgbm.LightGBMModel` - Wrapper around LightGBM&#39;s `LGBMRegressor`</span>
<span class="sd">* :class:`~darts.models.forecasting.lgbm.LightGBMClassifierModel` - Wrapper around LightGBM&#39;s `LGBMClassifier`</span>

<span class="sd">The wrappers come with all capabilities of Darts&#39; `SKLearn*Model`.</span>

<span class="sd">For detailed examples and tutorials, see:</span>

<span class="sd">* `SKLearn-Like Regression Model Examples &lt;https://unit8co.github.io/darts/examples/20-SKLearnModel-examples.html&gt;`_</span>
<span class="sd">* `SKLearn-Like Classification Model Examples &lt;https://unit8co.github.io/darts/examples/24-SKLearnClassifierModel-examples.html&gt;`_</span>

<span class="sd">To enable LightGBM support in Darts, follow the detailed install instructions for LightGBM in the INSTALL:</span>
<span class="sd">https://github.com/unit8co/darts/blob/master/INSTALL.md</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Sequence</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">darts</span><span class="w"> </span><span class="kn">import</span> <span class="n">TimeSeries</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">darts.logging</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_logger</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">darts.models.forecasting.sklearn_model</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">FUTURE_LAGS_TYPE</span><span class="p">,</span>
    <span class="n">LAGS_TYPE</span><span class="p">,</span>
    <span class="n">SKLearnModelWithCategoricalFeatures</span><span class="p">,</span>
    <span class="n">_ClassifierMixin</span><span class="p">,</span>
    <span class="n">_QuantileModelContainer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">darts.utils.likelihood_models.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">LikelihoodType</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">darts.utils.likelihood_models.sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantileRegression</span><span class="p">,</span> <span class="n">_get_likelihood</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="LightGBMModel"><a class="viewcode-back" href="../../../../generated_api/darts.models.forecasting.lgbm.html#darts.models.forecasting.lgbm.LightGBMModel">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">LightGBMModel</span><span class="p">(</span><span class="n">SKLearnModelWithCategoricalFeatures</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">lags</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LAGS_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lags_past_covariates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LAGS_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lags_future_covariates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FUTURE_LAGS_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_chunk_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">output_chunk_shift</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">add_encoders</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">likelihood</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantiles</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">multi_models</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">use_static_covariates</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">categorical_past_covariates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">categorical_future_covariates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">categorical_static_covariates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;LGBM Model</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        lags</span>
<span class="sd">            Lagged target `series` values used to predict the next time step/s.</span>
<span class="sd">            If an integer, must be &gt; 0. Uses the last `n=lags` past lags; e.g. `(-1, -2, ..., -lags)`, where `0`</span>
<span class="sd">            corresponds the first predicted time step of each sample. If `output_chunk_shift &gt; 0`, then</span>
<span class="sd">            lag `-1` translates to `-1 - output_chunk_shift` steps before the first prediction step.</span>
<span class="sd">            If a list of integers, each value must be &lt; 0. Uses only the specified values as lags.</span>
<span class="sd">            If a dictionary, the keys correspond to the `series` component names (of the first series when</span>
<span class="sd">            using multiple series) and the values correspond to the component lags (integer or list of integers). The</span>
<span class="sd">            key &#39;default_lags&#39; can be used to provide default lags for un-specified components. Raises and error if some</span>
<span class="sd">            components are missing and the &#39;default_lags&#39; key is not provided.</span>
<span class="sd">        lags_past_covariates</span>
<span class="sd">            Lagged `past_covariates` values used to predict the next time step/s.</span>
<span class="sd">            If an integer, must be &gt; 0. Uses the last `n=lags_past_covariates` past lags; e.g. `(-1, -2, ..., -lags)`,</span>
<span class="sd">            where `0` corresponds to the first predicted time step of each sample. If `output_chunk_shift &gt; 0`, then</span>
<span class="sd">            lag `-1` translates to `-1 - output_chunk_shift` steps before the first prediction step.</span>
<span class="sd">            If a list of integers, each value must be &lt; 0. Uses only the specified values as lags.</span>
<span class="sd">            If a dictionary, the keys correspond to the `past_covariates` component names (of the first series when</span>
<span class="sd">            using multiple series) and the values correspond to the component lags (integer or list of integers). The</span>
<span class="sd">            key &#39;default_lags&#39; can be used to provide default lags for un-specified components. Raises and error if some</span>
<span class="sd">            components are missing and the &#39;default_lags&#39; key is not provided.</span>
<span class="sd">        lags_future_covariates</span>
<span class="sd">            Lagged `future_covariates` values used to predict the next time step/s. The lags are always relative to the</span>
<span class="sd">            first step in the output chunk, even when `output_chunk_shift &gt; 0`.</span>
<span class="sd">            If a tuple of `(past, future)`, both values must be &gt; 0. Uses the last `n=past` past lags and `n=future`</span>
<span class="sd">            future lags; e.g. `(-past, -(past - 1), ..., -1, 0, 1, .... future - 1)`, where `0` corresponds the first</span>
<span class="sd">            predicted time step of each sample. If `output_chunk_shift &gt; 0`, the position of negative lags differ from</span>
<span class="sd">            those of `lags` and `lags_past_covariates`. In this case a future lag `-5` would point at the same</span>
<span class="sd">            step as a target lag of `-5 + output_chunk_shift`.</span>
<span class="sd">            If a list of integers, uses only the specified values as lags.</span>
<span class="sd">            If a dictionary, the keys correspond to the `future_covariates` component names (of the first series when</span>
<span class="sd">            using multiple series) and the values correspond to the component lags (tuple or list of integers). The key</span>
<span class="sd">            &#39;default_lags&#39; can be used to provide default lags for un-specified components. Raises and error if some</span>
<span class="sd">            components are missing and the &#39;default_lags&#39; key is not provided.</span>
<span class="sd">        output_chunk_length</span>
<span class="sd">            Number of time steps predicted at once (per chunk) by the internal model. It is not the same as forecast</span>
<span class="sd">            horizon `n` used in `predict()`, which is the desired number of prediction points generated using a</span>
<span class="sd">            one-shot- or autoregressive forecast. Setting `n &lt;= output_chunk_length` prevents auto-regression. This is</span>
<span class="sd">            useful when the covariates don&#39;t extend far enough into the future, or to prohibit the model from using</span>
<span class="sd">            future values of past and / or future covariates for prediction (depending on the model&#39;s covariate</span>
<span class="sd">            support).</span>
<span class="sd">        output_chunk_shift</span>
<span class="sd">            Optionally, the number of steps to shift the start of the output chunk into the future (relative to the</span>
<span class="sd">            input chunk end). This will create a gap between the input (history of target and past covariates) and</span>
<span class="sd">            output. If the model supports `future_covariates`, the `lags_future_covariates` are relative to the first</span>
<span class="sd">            step in the shifted output chunk. Predictions will start `output_chunk_shift` steps after the end of the</span>
<span class="sd">            target `series`. If `output_chunk_shift` is set, the model cannot generate autoregressive predictions</span>
<span class="sd">            (`n &gt; output_chunk_length`).</span>
<span class="sd">        add_encoders</span>
<span class="sd">            A large number of past and future covariates can be automatically generated with `add_encoders`.</span>
<span class="sd">            This can be done by adding multiple pre-defined index encoders and/or custom user-made functions that</span>
<span class="sd">            will be used as index encoders. Additionally, a transformer such as Darts&#39; :class:`Scaler` can be added to</span>
<span class="sd">            transform the generated covariates. This happens all under one hood and only needs to be specified at</span>
<span class="sd">            model creation.</span>
<span class="sd">            Read :meth:`SequentialEncoder &lt;darts.dataprocessing.encoders.SequentialEncoder&gt;` to find out more about</span>
<span class="sd">            ``add_encoders``. Default: ``None``. An example showing some of ``add_encoders`` features:</span>

<span class="sd">            .. highlight:: python</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                def encode_year(idx):</span>
<span class="sd">                    return (idx.year - 1950) / 50</span>

<span class="sd">                add_encoders={</span>
<span class="sd">                    &#39;cyclic&#39;: {&#39;future&#39;: [&#39;month&#39;]},</span>
<span class="sd">                    &#39;datetime_attribute&#39;: {&#39;future&#39;: [&#39;hour&#39;, &#39;dayofweek&#39;]},</span>
<span class="sd">                    &#39;position&#39;: {&#39;past&#39;: [&#39;relative&#39;], &#39;future&#39;: [&#39;relative&#39;]},</span>
<span class="sd">                    &#39;custom&#39;: {&#39;past&#39;: [encode_year]},</span>
<span class="sd">                    &#39;transformer&#39;: Scaler(),</span>
<span class="sd">                    &#39;tz&#39;: &#39;CET&#39;</span>
<span class="sd">                }</span>
<span class="sd">            ..</span>
<span class="sd">        likelihood</span>
<span class="sd">            Can be set to `quantile` or `poisson`. If set, the model will be probabilistic, allowing sampling at</span>
<span class="sd">            prediction time. This will overwrite any `objective` parameter.</span>
<span class="sd">        quantiles</span>
<span class="sd">            Fit the model to these quantiles if the `likelihood` is set to `quantile`.</span>
<span class="sd">        random_state</span>
<span class="sd">            Controls the randomness for reproducible forecasting.</span>
<span class="sd">        multi_models</span>
<span class="sd">            If True, a separate model will be trained for each future lag to predict. If False, a single model</span>
<span class="sd">            is trained to predict all the steps in &#39;output_chunk_length&#39; (features lags are shifted back by</span>
<span class="sd">            `output_chunk_length - n` for each step `n`). Default: True.</span>
<span class="sd">        use_static_covariates</span>
<span class="sd">            Whether the model should use static covariate information in case the input `series` passed to ``fit()``</span>
<span class="sd">            contain static covariates. If ``True``, and static covariates are available at fitting time, will enforce</span>
<span class="sd">            that all target `series` have the same static covariate dimensionality in ``fit()`` and ``predict()``.</span>
<span class="sd">        categorical_past_covariates</span>
<span class="sd">            Optionally, component name or list of component names specifying the past covariates that should be treated</span>
<span class="sd">            as categorical by the underlying `lightgbm.LightGBMRegressor`. The components that are specified as</span>
<span class="sd">            categorical must be integer-encoded. For more information on how LightGBM handles categorical</span>
<span class="sd">            features, visit: `Categorical feature support documentation</span>
<span class="sd">            &lt;https://lightgbm.readthedocs.io/en/latest/Features.html#optimal-split-for-categorical-features&gt;`_.</span>
<span class="sd">        categorical_future_covariates</span>
<span class="sd">            Optionally, component name or list of component names specifying the future covariates that should be</span>
<span class="sd">            treated as categorical by the underlying `lightgbm.LightGBMRegressor`. The components that</span>
<span class="sd">            are specified as categorical must be integer-encoded.</span>
<span class="sd">        categorical_static_covariates</span>
<span class="sd">            Optionally, string or list of strings specifying the static covariates that should be treated as categorical</span>
<span class="sd">            by the underlying `lightgbm.LightGBMRegressor`. The components that are specified as categorical</span>
<span class="sd">            must be integer-encoded.</span>
<span class="sd">        **kwargs</span>
<span class="sd">            Additional keyword arguments passed to `lightgbm.LGBRegressor`.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; from darts.datasets import WeatherDataset</span>
<span class="sd">        &gt;&gt;&gt; from darts.models import LightGBMModel</span>
<span class="sd">        &gt;&gt;&gt; series = WeatherDataset().load()</span>
<span class="sd">        &gt;&gt;&gt; # predicting atmospheric pressure</span>
<span class="sd">        &gt;&gt;&gt; target = series[&#39;p (mbar)&#39;][:100]</span>
<span class="sd">        &gt;&gt;&gt; # optionally, use past observed rainfall (pretending to be unknown beyond index 100)</span>
<span class="sd">        &gt;&gt;&gt; past_cov = series[&#39;rain (mm)&#39;][:100]</span>
<span class="sd">        &gt;&gt;&gt; # optionally, use future temperatures (pretending this component is a forecast)</span>
<span class="sd">        &gt;&gt;&gt; future_cov = series[&#39;T (degC)&#39;][:106]</span>
<span class="sd">        &gt;&gt;&gt; # predict 6 pressure values using the 12 past values of pressure and rainfall, as well as the 6 temperature</span>
<span class="sd">        &gt;&gt;&gt; # values corresponding to the forecasted period</span>
<span class="sd">        &gt;&gt;&gt; model = LightGBMModel(</span>
<span class="sd">        &gt;&gt;&gt;     lags=12,</span>
<span class="sd">        &gt;&gt;&gt;     lags_past_covariates=12,</span>
<span class="sd">        &gt;&gt;&gt;     lags_future_covariates=[0,1,2,3,4,5],</span>
<span class="sd">        &gt;&gt;&gt;     output_chunk_length=6,</span>
<span class="sd">        &gt;&gt;&gt;     verbose=-1</span>
<span class="sd">        &gt;&gt;&gt; )</span>
<span class="sd">        &gt;&gt;&gt; model.fit(target, past_covariates=past_cov, future_covariates=future_cov)</span>
<span class="sd">        &gt;&gt;&gt; pred = model.predict(6)</span>
<span class="sd">        &gt;&gt;&gt; pred.values()</span>
<span class="sd">        array([[1006.85376674],</span>
<span class="sd">               [1006.83998586],</span>
<span class="sd">               [1006.63884831],</span>
<span class="sd">               [1006.57201255],</span>
<span class="sd">               [1006.52290556],</span>
<span class="sd">               [1006.39550065]])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;random_state&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">random_state</span>  <span class="c1"># seed for tree learner</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_set_likelihood</span><span class="p">(</span>
            <span class="n">likelihood</span><span class="o">=</span><span class="n">likelihood</span><span class="p">,</span>
            <span class="n">output_chunk_length</span><span class="o">=</span><span class="n">output_chunk_length</span><span class="p">,</span>
            <span class="n">multi_models</span><span class="o">=</span><span class="n">multi_models</span><span class="p">,</span>
            <span class="n">quantiles</span><span class="o">=</span><span class="n">quantiles</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">lags</span><span class="o">=</span><span class="n">lags</span><span class="p">,</span>
            <span class="n">lags_past_covariates</span><span class="o">=</span><span class="n">lags_past_covariates</span><span class="p">,</span>
            <span class="n">lags_future_covariates</span><span class="o">=</span><span class="n">lags_future_covariates</span><span class="p">,</span>
            <span class="n">output_chunk_length</span><span class="o">=</span><span class="n">output_chunk_length</span><span class="p">,</span>
            <span class="n">output_chunk_shift</span><span class="o">=</span><span class="n">output_chunk_shift</span><span class="p">,</span>
            <span class="n">add_encoders</span><span class="o">=</span><span class="n">add_encoders</span><span class="p">,</span>
            <span class="n">multi_models</span><span class="o">=</span><span class="n">multi_models</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_create_model</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">),</span>
            <span class="n">use_static_covariates</span><span class="o">=</span><span class="n">use_static_covariates</span><span class="p">,</span>
            <span class="n">categorical_past_covariates</span><span class="o">=</span><span class="n">categorical_past_covariates</span><span class="p">,</span>
            <span class="n">categorical_future_covariates</span><span class="o">=</span><span class="n">categorical_future_covariates</span><span class="p">,</span>
            <span class="n">categorical_static_covariates</span><span class="o">=</span><span class="n">categorical_static_covariates</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_create_model</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">lgb</span><span class="o">.</span><span class="n">LGBMRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_set_likelihood</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">likelihood</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">output_chunk_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">multi_models</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">quantiles</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_likelihood</span> <span class="o">=</span> <span class="n">_get_likelihood</span><span class="p">(</span>
            <span class="n">likelihood</span><span class="o">=</span><span class="n">likelihood</span><span class="p">,</span>
            <span class="n">n_outputs</span><span class="o">=</span><span class="n">output_chunk_length</span> <span class="k">if</span> <span class="n">multi_models</span> <span class="k">else</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">quantiles</span><span class="o">=</span><span class="n">quantiles</span><span class="p">,</span>
            <span class="n">available_likelihoods</span><span class="o">=</span><span class="p">[</span><span class="n">LikelihoodType</span><span class="o">.</span><span class="n">Quantile</span><span class="p">,</span> <span class="n">LikelihoodType</span><span class="o">.</span><span class="n">Poisson</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">likelihood</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;objective&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">likelihood</span>
        <span class="k">if</span> <span class="n">likelihood</span> <span class="o">==</span> <span class="n">LikelihoodType</span><span class="o">.</span><span class="n">Quantile</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_model_container</span> <span class="o">=</span> <span class="n">_QuantileModelContainer</span><span class="p">()</span>

<div class="viewcode-block" id="LightGBMModel.fit"><a class="viewcode-back" href="../../../../generated_api/darts.models.forecasting.lgbm.html#darts.models.forecasting.lgbm.LightGBMModel.fit">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">series</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">]],</span>
        <span class="n">past_covariates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">future_covariates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">val_series</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">val_past_covariates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">val_future_covariates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_samples_per_ts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_jobs_multioutput_wrapper</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">val_sample_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">TimeSeries</span><span class="p">],</span> <span class="nb">str</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fits/trains the model using the provided list of features time series and the target time series.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        series</span>
<span class="sd">            TimeSeries or Sequence[TimeSeries] object containing the target values.</span>
<span class="sd">        past_covariates</span>
<span class="sd">            Optionally, a series or sequence of series specifying past-observed covariates</span>
<span class="sd">        future_covariates</span>
<span class="sd">            Optionally, a series or sequence of series specifying future-known covariates</span>
<span class="sd">        val_series</span>
<span class="sd">            TimeSeries or Sequence[TimeSeries] object containing the target values for evaluation dataset</span>
<span class="sd">        val_past_covariates</span>
<span class="sd">            Optionally, a series or sequence of series specifying past-observed covariates for evaluation dataset</span>
<span class="sd">        val_future_covariates : Union[TimeSeries, Sequence[TimeSeries]]</span>
<span class="sd">            Optionally, a series or sequence of series specifying future-known covariates for evaluation dataset</span>
<span class="sd">        max_samples_per_ts</span>
<span class="sd">            This is an integer upper bound on the number of tuples that can be produced</span>
<span class="sd">            per time series. It can be used in order to have an upper bound on the total size of the dataset and</span>
<span class="sd">            ensure proper sampling. If `None`, it will read all of the individual time series in advance (at dataset</span>
<span class="sd">            creation) to know their sizes, which might be expensive on big datasets.</span>
<span class="sd">            If some series turn out to have a length that would allow more than `max_samples_per_ts`, only the</span>
<span class="sd">            most recent `max_samples_per_ts` samples will be considered.</span>
<span class="sd">        n_jobs_multioutput_wrapper</span>
<span class="sd">            Number of jobs of the MultiOutputRegressor wrapper to run in parallel. Only used if the model doesn&#39;t</span>
<span class="sd">            support multi-output regression natively.</span>
<span class="sd">        sample_weight</span>
<span class="sd">            Optionally, some sample weights to apply to the target `series` labels. They are applied per observation,</span>
<span class="sd">            per label (each step in `output_chunk_length`), and per component.</span>
<span class="sd">            If a series or sequence of series, then those weights are used. If the weight series only have a single</span>
<span class="sd">            component / column, then the weights are applied globally to all components in `series`. Otherwise, for</span>
<span class="sd">            component-specific weights, the number of components must match those of `series`.</span>
<span class="sd">            If a string, then the weights are generated using built-in weighting functions. The available options are</span>
<span class="sd">            `&quot;linear&quot;` or `&quot;exponential&quot;` decay - the further in the past, the lower the weight. The weights are</span>
<span class="sd">            computed globally based on the length of the longest series in `series`. Then for each series, the weights</span>
<span class="sd">            are extracted from the end of the global weights. This gives a common time weighting across all series.</span>
<span class="sd">        val_sample_weight</span>
<span class="sd">            Same as for `sample_weight` but for the evaluation dataset.</span>
<span class="sd">         **kwargs</span>
<span class="sd">            Additional kwargs passed to `lightgbm.LGBRegressor.fit()`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">likelihood</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">QuantileRegression</span><span class="p">):</span>
            <span class="c1"># empty model container in case of multiple calls to fit, e.g. when backtesting</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_model_container</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">quantile</span> <span class="ow">in</span> <span class="n">likelihood</span><span class="o">.</span><span class="n">quantiles</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;alpha&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">quantile</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_model</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                    <span class="n">series</span><span class="o">=</span><span class="n">series</span><span class="p">,</span>
                    <span class="n">past_covariates</span><span class="o">=</span><span class="n">past_covariates</span><span class="p">,</span>
                    <span class="n">future_covariates</span><span class="o">=</span><span class="n">future_covariates</span><span class="p">,</span>
                    <span class="n">val_series</span><span class="o">=</span><span class="n">val_series</span><span class="p">,</span>
                    <span class="n">val_past_covariates</span><span class="o">=</span><span class="n">val_past_covariates</span><span class="p">,</span>
                    <span class="n">val_future_covariates</span><span class="o">=</span><span class="n">val_future_covariates</span><span class="p">,</span>
                    <span class="n">max_samples_per_ts</span><span class="o">=</span><span class="n">max_samples_per_ts</span><span class="p">,</span>
                    <span class="n">n_jobs_multioutput_wrapper</span><span class="o">=</span><span class="n">n_jobs_multioutput_wrapper</span><span class="p">,</span>
                    <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                    <span class="n">val_sample_weight</span><span class="o">=</span><span class="n">val_sample_weight</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="c1"># store the trained model in the container as it might have been wrapped by MultiOutputRegressor</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_model_container</span><span class="p">[</span><span class="n">quantile</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>
            <span class="k">return</span> <span class="bp">self</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="n">series</span><span class="o">=</span><span class="n">series</span><span class="p">,</span>
            <span class="n">past_covariates</span><span class="o">=</span><span class="n">past_covariates</span><span class="p">,</span>
            <span class="n">future_covariates</span><span class="o">=</span><span class="n">future_covariates</span><span class="p">,</span>
            <span class="n">val_series</span><span class="o">=</span><span class="n">val_series</span><span class="p">,</span>
            <span class="n">val_past_covariates</span><span class="o">=</span><span class="n">val_past_covariates</span><span class="p">,</span>
            <span class="n">val_future_covariates</span><span class="o">=</span><span class="n">val_future_covariates</span><span class="p">,</span>
            <span class="n">max_samples_per_ts</span><span class="o">=</span><span class="n">max_samples_per_ts</span><span class="p">,</span>
            <span class="n">n_jobs_multioutput_wrapper</span><span class="o">=</span><span class="n">n_jobs_multioutput_wrapper</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
            <span class="n">val_sample_weight</span><span class="o">=</span><span class="n">val_sample_weight</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">supports_val_set</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">val_set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
        <span class="k">return</span> <span class="s2">&quot;eval_set&quot;</span><span class="p">,</span> <span class="s2">&quot;eval_sample_weight&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">min_train_series_length</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="c1"># LightGBM requires a minimum of 2 train samples, therefore the min_train_series_length should be one more than</span>
        <span class="c1"># for other regression models</span>
        <span class="k">return</span> <span class="nb">max</span><span class="p">(</span>
            <span class="mi">3</span><span class="p">,</span>
            <span class="p">(</span>
                <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">lags</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_chunk_length</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="s2">&quot;target&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lags</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_chunk_length</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_categorical_fit_param</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the name of the categorical features parameter from model&#39;s `fit` method .</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;categorical_feature&quot;</span></div>


<div class="viewcode-block" id="LightGBMClassifierModel"><a class="viewcode-back" href="../../../../generated_api/darts.models.forecasting.lgbm.html#darts.models.forecasting.lgbm.LightGBMClassifierModel">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">LightGBMClassifierModel</span><span class="p">(</span><span class="n">_ClassifierMixin</span><span class="p">,</span> <span class="n">LightGBMModel</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">lags</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lags_past_covariates</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lags_future_covariates</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_chunk_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">output_chunk_shift</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">add_encoders</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">likelihood</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">LikelihoodType</span><span class="o">.</span><span class="n">ClassProbability</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">multi_models</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">use_static_covariates</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">categorical_past_covariates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">categorical_future_covariates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">categorical_static_covariates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;LGBM Model for classification forecasting</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        lags</span>
<span class="sd">            Lagged target `series` values used to predict the next time step/s.</span>
<span class="sd">            If an integer, must be &gt; 0. Uses the last `n=lags` past lags; e.g. `(-1, -2, ..., -lags)`, where `0`</span>
<span class="sd">            corresponds the first predicted time step of each sample. If `output_chunk_shift &gt; 0`, then</span>
<span class="sd">            lag `-1` translates to `-1 - output_chunk_shift` steps before the first prediction step.</span>
<span class="sd">            If a list of integers, each value must be &lt; 0. Uses only the specified values as lags.</span>
<span class="sd">            If a dictionary, the keys correspond to the `series` component names (of the first series when</span>
<span class="sd">            using multiple series) and the values correspond to the component lags (integer or list of integers). The</span>
<span class="sd">            key &#39;default_lags&#39; can be used to provide default lags for un-specified components. Raises and error if some</span>
<span class="sd">            components are missing and the &#39;default_lags&#39; key is not provided.</span>
<span class="sd">            This model treats the target `series` as categorical features when lags are provided.</span>
<span class="sd">        lags_past_covariates</span>
<span class="sd">            Lagged `past_covariates` values used to predict the next time step/s.</span>
<span class="sd">            If an integer, must be &gt; 0. Uses the last `n=lags_past_covariates` past lags; e.g. `(-1, -2, ..., -lags)`,</span>
<span class="sd">            where `0` corresponds to the first predicted time step of each sample. If `output_chunk_shift &gt; 0`, then</span>
<span class="sd">            lag `-1` translates to `-1 - output_chunk_shift` steps before the first prediction step.</span>
<span class="sd">            If a list of integers, each value must be &lt; 0. Uses only the specified values as lags.</span>
<span class="sd">            If a dictionary, the keys correspond to the `past_covariates` component names (of the first series when</span>
<span class="sd">            using multiple series) and the values correspond to the component lags (integer or list of integers). The</span>
<span class="sd">            key &#39;default_lags&#39; can be used to provide default lags for un-specified components. Raises and error if some</span>
<span class="sd">            components are missing and the &#39;default_lags&#39; key is not provided.</span>
<span class="sd">        lags_future_covariates</span>
<span class="sd">            Lagged `future_covariates` values used to predict the next time step/s. The lags are always relative to the</span>
<span class="sd">            first step in the output chunk, even when `output_chunk_shift &gt; 0`.</span>
<span class="sd">            If a tuple of `(past, future)`, both values must be &gt; 0. Uses the last `n=past` past lags and `n=future`</span>
<span class="sd">            future lags; e.g. `(-past, -(past - 1), ..., -1, 0, 1, .... future - 1)`, where `0` corresponds the first</span>
<span class="sd">            predicted time step of each sample. If `output_chunk_shift &gt; 0`, the position of negative lags differ from</span>
<span class="sd">            those of `lags` and `lags_past_covariates`. In this case a future lag `-5` would point at the same</span>
<span class="sd">            step as a target lag of `-5 + output_chunk_shift`.</span>
<span class="sd">            If a list of integers, uses only the specified values as lags.</span>
<span class="sd">            If a dictionary, the keys correspond to the `future_covariates` component names (of the first series when</span>
<span class="sd">            using multiple series) and the values correspond to the component lags (tuple or list of integers). The key</span>
<span class="sd">            &#39;default_lags&#39; can be used to provide default lags for un-specified components. Raises and error if some</span>
<span class="sd">            components are missing and the &#39;default_lags&#39; key is not provided.</span>
<span class="sd">        output_chunk_length</span>
<span class="sd">            Number of time steps predicted at once (per chunk) by the internal model. It is not the same as forecast</span>
<span class="sd">            horizon `n` used in `predict()`, which is the desired number of prediction points generated using a</span>
<span class="sd">            one-shot- or autoregressive forecast. Setting `n &lt;= output_chunk_length` prevents auto-regression. This is</span>
<span class="sd">            useful when the covariates don&#39;t extend far enough into the future, or to prohibit the model from using</span>
<span class="sd">            future values of past and / or future covariates for prediction (depending on the model&#39;s covariate</span>
<span class="sd">            support).</span>
<span class="sd">        output_chunk_shift</span>
<span class="sd">            Optionally, the number of steps to shift the start of the output chunk into the future (relative to the</span>
<span class="sd">            input chunk end). This will create a gap between the input (history of target and past covariates) and</span>
<span class="sd">            output. If the model supports `future_covariates`, the `lags_future_covariates` are relative to the first</span>
<span class="sd">            step in the shifted output chunk. Predictions will start `output_chunk_shift` steps after the end of the</span>
<span class="sd">            target `series`. If `output_chunk_shift` is set, the model cannot generate autoregressive predictions</span>
<span class="sd">            (`n &gt; output_chunk_length`).</span>
<span class="sd">        add_encoders</span>
<span class="sd">            A large number of past and future covariates can be automatically generated with `add_encoders`.</span>
<span class="sd">            This can be done by adding multiple pre-defined index encoders and/or custom user-made functions that</span>
<span class="sd">            will be used as index encoders. Additionally, a transformer such as Darts&#39; :class:`Scaler` can be added to</span>
<span class="sd">            transform the generated covariates. This happens all under one hood and only needs to be specified at</span>
<span class="sd">            model creation.</span>
<span class="sd">            Read :meth:`SequentialEncoder &lt;darts.dataprocessing.encoders.SequentialEncoder&gt;` to find out more about</span>
<span class="sd">            ``add_encoders``. Default: ``None``. An example showing some of ``add_encoders`` features:</span>

<span class="sd">            .. highlight:: python</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                def encode_year(idx):</span>
<span class="sd">                    return (idx.year - 1950) / 50</span>

<span class="sd">                add_encoders={</span>
<span class="sd">                    &#39;cyclic&#39;: {&#39;future&#39;: [&#39;month&#39;]},</span>
<span class="sd">                    &#39;datetime_attribute&#39;: {&#39;future&#39;: [&#39;hour&#39;, &#39;dayofweek&#39;]},</span>
<span class="sd">                    &#39;position&#39;: {&#39;past&#39;: [&#39;relative&#39;], &#39;future&#39;: [&#39;relative&#39;]},</span>
<span class="sd">                    &#39;custom&#39;: {&#39;past&#39;: [encode_year]},</span>
<span class="sd">                    &#39;transformer&#39;: Scaler(),</span>
<span class="sd">                    &#39;tz&#39;: &#39;CET&#39;</span>
<span class="sd">                }</span>
<span class="sd">            ..</span>
<span class="sd">        likelihood</span>
<span class="sd">            &#39;classprobability&#39; or ``None``. If set to &#39;classprobability&#39;, setting `predict_likelihood_parameters`</span>
<span class="sd">            in `predict()` will forecast class probabilities.</span>
<span class="sd">            Default: &#39;classprobability&#39;</span>
<span class="sd">        random_state</span>
<span class="sd">            Controls the randomness for reproducible forecasting.</span>
<span class="sd">        multi_models</span>
<span class="sd">            If True, a separate model will be trained for each future lag to predict. If False, a single model</span>
<span class="sd">            is trained to predict all the steps in &#39;output_chunk_length&#39; (features lags are shifted back by</span>
<span class="sd">            `output_chunk_length - n` for each step `n`). Default: True.</span>
<span class="sd">        use_static_covariates</span>
<span class="sd">            Whether the model should use static covariate information in case the input `series` passed to ``fit()``</span>
<span class="sd">            contain static covariates. If ``True``, and static covariates are available at fitting time, will enforce</span>
<span class="sd">            that all target `series` have the same static covariate dimensionality in ``fit()`` and ``predict()``.</span>
<span class="sd">        categorical_past_covariates</span>
<span class="sd">            Optionally, component name or list of component names specifying the past covariates that should be treated</span>
<span class="sd">            as categorical by the underlying `lightgbm.LightGBMRegressor`. The components that are specified as</span>
<span class="sd">            categorical must be integer-encoded. For more information on how LightGBM handles categorical</span>
<span class="sd">            features, visit: `Categorical feature support documentation</span>
<span class="sd">            &lt;https://lightgbm.readthedocs.io/en/latest/Features.html#optimal-split-for-categorical-features&gt;`_.</span>
<span class="sd">        categorical_future_covariates</span>
<span class="sd">            Optionally, component name or list of component names specifying the future covariates that should be</span>
<span class="sd">            treated as categorical by the underlying `lightgbm.LightGBMRegressor`. The components that</span>
<span class="sd">            are specified as categorical must be integer-encoded.</span>
<span class="sd">        categorical_static_covariates</span>
<span class="sd">            Optionally, string or list of strings specifying the static covariates that should be treated as categorical</span>
<span class="sd">            by the underlying `lightgbm.LightGBMRegressor`. The components that are specified as categorical</span>
<span class="sd">            must be integer-encoded.</span>
<span class="sd">        **kwargs</span>
<span class="sd">            Additional keyword arguments passed to `lightgbm.LGBClassifier`.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from darts.datasets import WeatherDataset</span>
<span class="sd">        &gt;&gt;&gt; from darts.models import LightGBMClassifierModel</span>
<span class="sd">        &gt;&gt;&gt; series = WeatherDataset().load().resample(&quot;1D&quot;, method=&quot;mean&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # predicting if it will rain or not</span>
<span class="sd">        &gt;&gt;&gt; target =  series[&#39;rain (mm)&#39;][:105].map(lambda x: np.where(x &gt; 0, 1, 0))</span>
<span class="sd">        &gt;&gt;&gt; # optionally, use past observed rainfall (pretending to be unknown beyond index 105)</span>
<span class="sd">        &gt;&gt;&gt; past_cov = series[&#39;T (degC)&#39;][:105]</span>
<span class="sd">        &gt;&gt;&gt; # optionally, use future pressure (pretending this component is a forecast)</span>
<span class="sd">        &gt;&gt;&gt; future_cov = series[&#39;p (mbar)&#39;][:111]</span>
<span class="sd">        &gt;&gt;&gt; # predict 6 &quot;will rain&quot; values using the 12 past values of pressure and temperature,</span>
<span class="sd">        &gt;&gt;&gt; # as well as the 6 pressure values corresponding to the forecasted period</span>
<span class="sd">        &gt;&gt;&gt; model = LightGBMClassifierModel(</span>
<span class="sd">        &gt;&gt;&gt;     lags=12,</span>
<span class="sd">        &gt;&gt;&gt;     lags_past_covariates=12,</span>
<span class="sd">        &gt;&gt;&gt;     lags_future_covariates=[0,1,2,3,4,5],</span>
<span class="sd">        &gt;&gt;&gt;     output_chunk_length=6,</span>
<span class="sd">        &gt;&gt;&gt;     verbose=-1</span>
<span class="sd">        &gt;&gt;&gt; )</span>
<span class="sd">        &gt;&gt;&gt; model.fit(target, past_covariates=past_cov, future_covariates=future_cov)</span>
<span class="sd">        &gt;&gt;&gt; pred = model.predict(6)</span>
<span class="sd">        &gt;&gt;&gt; pred.values()</span>
<span class="sd">        array([[0.],</span>
<span class="sd">               [0.],</span>
<span class="sd">               [0.],</span>
<span class="sd">               [1.],</span>
<span class="sd">               [1.],</span>
<span class="sd">               [0.]])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># likelihood always set to ClassProbability as it&#39;s the only supported classifiaction likelihood</span>
        <span class="c1"># this allow users to predict class probabilities,</span>
        <span class="c1"># by setting `predict_likelihood_parameters`to `True` in `predict()`</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">lags</span><span class="o">=</span><span class="n">lags</span><span class="p">,</span>
            <span class="n">lags_past_covariates</span><span class="o">=</span><span class="n">lags_past_covariates</span><span class="p">,</span>
            <span class="n">lags_future_covariates</span><span class="o">=</span><span class="n">lags_future_covariates</span><span class="p">,</span>
            <span class="n">output_chunk_length</span><span class="o">=</span><span class="n">output_chunk_length</span><span class="p">,</span>
            <span class="n">output_chunk_shift</span><span class="o">=</span><span class="n">output_chunk_shift</span><span class="p">,</span>
            <span class="n">add_encoders</span><span class="o">=</span><span class="n">add_encoders</span><span class="p">,</span>
            <span class="n">likelihood</span><span class="o">=</span><span class="n">likelihood</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">multi_models</span><span class="o">=</span><span class="n">multi_models</span><span class="p">,</span>
            <span class="n">use_static_covariates</span><span class="o">=</span><span class="n">use_static_covariates</span><span class="p">,</span>
            <span class="n">categorical_past_covariates</span><span class="o">=</span><span class="n">categorical_past_covariates</span><span class="p">,</span>
            <span class="n">categorical_future_covariates</span><span class="o">=</span><span class="n">categorical_future_covariates</span><span class="p">,</span>
            <span class="n">categorical_static_covariates</span><span class="o">=</span><span class="n">categorical_static_covariates</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_create_model</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">lgb</span><span class="o">.</span><span class="n">LGBMClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_set_likelihood</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">likelihood</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">output_chunk_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">multi_models</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">quantiles</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check and set the likelihood.</span>
<span class="sd">        Only ClassProbability is supported for LightGBMClassifierModel.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_likelihood</span> <span class="o">=</span> <span class="n">_get_likelihood</span><span class="p">(</span>
            <span class="n">likelihood</span><span class="o">=</span><span class="n">likelihood</span><span class="p">,</span>
            <span class="n">n_outputs</span><span class="o">=</span><span class="n">output_chunk_length</span> <span class="k">if</span> <span class="n">multi_models</span> <span class="k">else</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">available_likelihoods</span><span class="o">=</span><span class="p">[</span><span class="n">LikelihoodType</span><span class="o">.</span><span class="n">ClassProbability</span><span class="p">],</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_supports_native_multioutput</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="c1"># LightGBM does not support multiclass natively currently (4.6.0)</span>
        <span class="k">return</span> <span class="kc">False</span></div>
</pre></div>

              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2020 - 2025, Unit8 SA (Apache 2.0 License).<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 5.0.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>